{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11a9669e-c92d-45ae-81b7-b59e2e8523af",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The previous stage includes reviews text preprocessing, which is in Process_Reviews.ipynb<br>\n",
    "The following stage is aim to analysis reviews sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9dbb51f-8804-4366-87b7-f9a3bc567865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\AA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\AA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\AA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
    "import seaborn as sns\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from gensim.models import Word2Vec\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import string\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2882f14-c15d-49e7-844b-49aa4a14dab9",
   "metadata": {},
   "source": [
    "# Load reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "788b8514-5d9e-442a-b96c-11e5d8bf35ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gt_h5A-A5Nt50EbaPCorvw</td>\n",
       "      <td>F8yozE3NWnImNApHO347gQ</td>\n",
       "      <td>First impression was good but their food is ho...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>impression good food horrible cash idea pricey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0dELc-isYD5Av7KNsIvcRA</td>\n",
       "      <td>WxB8498ejPtHE7wFa89_fA</td>\n",
       "      <td>I've been to this location countless times, an...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>location countless time time food service ambi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bmrq4dvmlVMGU3roXzeDgQ</td>\n",
       "      <td>-mIlmp5l4hKlp1tvHRdvTg</td>\n",
       "      <td>Seems popular- but not that delish. Short sub ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>popular delish short sub maybe typical length ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tCP2EjtzGJ7JEHlj_8i1xw</td>\n",
       "      <td>hcxea89M_U__LADtu3C0kA</td>\n",
       "      <td>The service was great! The place was very beau...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>service great place beautiful small tight rest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3gM_kcsqfU9eqmE19kL4tw</td>\n",
       "      <td>3BJxm-HnvzdwD1zjmSbmyQ</td>\n",
       "      <td>I was at this restaurant recently and was so u...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>restaurant recently unhappy purchase wonton so...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id             business_id  \\\n",
       "0  Gt_h5A-A5Nt50EbaPCorvw  F8yozE3NWnImNApHO347gQ   \n",
       "1  0dELc-isYD5Av7KNsIvcRA  WxB8498ejPtHE7wFa89_fA   \n",
       "2  Bmrq4dvmlVMGU3roXzeDgQ  -mIlmp5l4hKlp1tvHRdvTg   \n",
       "3  tCP2EjtzGJ7JEHlj_8i1xw  hcxea89M_U__LADtu3C0kA   \n",
       "4  3gM_kcsqfU9eqmE19kL4tw  3BJxm-HnvzdwD1zjmSbmyQ   \n",
       "\n",
       "                                                text  target  \\\n",
       "0  First impression was good but their food is ho...     0.0   \n",
       "1  I've been to this location countless times, an...     0.0   \n",
       "2  Seems popular- but not that delish. Short sub ...     0.0   \n",
       "3  The service was great! The place was very beau...     0.0   \n",
       "4  I was at this restaurant recently and was so u...     0.0   \n",
       "\n",
       "                                               words  \n",
       "0     impression good food horrible cash idea pricey  \n",
       "1  location countless time time food service ambi...  \n",
       "2  popular delish short sub maybe typical length ...  \n",
       "3  service great place beautiful small tight rest...  \n",
       "4  restaurant recently unhappy purchase wonton so...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"Cleaned_Text_Dataset.csv\"\n",
    "df = pd.read_csv(file)\n",
    "del df[\"Unnamed: 0\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078c8fdd-8a0d-4442-a015-8af277852562",
   "metadata": {},
   "source": [
    "# Sentiment polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2cd247-9cb6-46e4-b248-173d6783995a",
   "metadata": {},
   "source": [
    "我们将情绪分为四个类别，negative、neutral、positive and compound。【摘抄】The first three are easy to understand and for the compound score, it is a combination of positive and negative scores and ranges from -1 to 1: below 0 is negative and above 0 is positive. I am going to use the compound score to measure the sentiment.前三个很容易理解，对于复合分数，它是正分数和负分数的组合，范围从 -1 到 1：低于 0 为负，高于 0 为正。我将使用复合分数来衡量情绪。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34ee8ad8-2b95-4007-88e3-16e46451104c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impression good food horrible cash idea pricey\n",
      "0    {'neg': 0.285, 'neu': 0.325, 'pos': 0.39, 'com...\n",
      "1    {'neg': 0.034, 'neu': 0.694, 'pos': 0.271, 'co...\n",
      "2    {'neg': 0.059, 'neu': 0.46, 'pos': 0.481, 'com...\n",
      "3    {'neg': 0.21, 'neu': 0.405, 'pos': 0.385, 'com...\n",
      "4    {'neg': 0.169, 'neu': 0.637, 'pos': 0.195, 'co...\n",
      "Name: words, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Instantiate new SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "# Generate sentiment scores\n",
    "sentiment_scores = df['words'].apply(sid.polarity_scores)\n",
    "sentiment = sentiment_scores.apply(lambda x: x['compound'])\n",
    "print(df['words'][0])\n",
    "print(sentiment_scores.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de3e6d1-391c-4aca-8288-f666f35d1170",
   "metadata": {},
   "source": [
    "# Split training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "787f4fcf-c4b2-4ca8-9038-f0e62d17d953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      " 4882    similar reviewer avoid place year simply veget...\n",
      "4931    happy pop major change work long table got rid...\n",
      "275     month track burger fi like iroquois brave prov...\n",
      "6592    husband dine wedding anniversary note occasion...\n",
      "7150    love reading terminal shop fish head pick hoag...\n",
      "Name: words, dtype: object\n",
      "\n",
      "y_train:\n",
      " 4882    1.0\n",
      "4931    1.0\n",
      "275     0.0\n",
      "6592    1.0\n",
      "7150    1.0\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X = df['words'] \n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify=y)\n",
    "print(\"X_train:\\n\", X_train.head())\n",
    "print(\"\\ny_train:\\n\", y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ff9c5-114e-4eaa-be38-904bbd8772b0",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fafc8d5-64e6-41dd-85e8-50609b72edbd",
   "metadata": {},
   "source": [
    "列举我们将要实验的n-gram，【摘抄】GridSearchCV是Sklearn model_selection包的一个模块，用于超参数调整。 给定一组不同的超参数，GridSearchCV 循环浏览所有可能的超参数值和组合，并在训练数据集上拟合模型。 在这个过程中，它能够确定产生最佳精度的超参数的最佳值和组合（从给定的参数集中）【摘抄】在机器学习模型中，需要人工选择的参数称为超参数。比如随机森林中决策树的个数，人工神经网络模型中隐藏层层数和每层的节点个数，正则项中常数大小等等，他们都需要事先指定。超参数选择不恰当，就会出现欠拟合或者过拟合的问题。而在选择超参数的时候，有两个途径，一个是凭经验微调，另一个就是选择不同大小的参数，带入模型中，挑选表现最好的参数。微调的一种方法是手工调制超参数，直到找到一个好的超参数组合，这么做的话会非常冗长，你也可能没有时间探索多种组合，所以可以使用Scikit-Learn的GridSearchCV来做这项搜索工作。<br>\n",
    "这里用的是后者<br>\n",
    "可以提一下交叉验证，cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fff587b-48fd-4467-9fc2-93b96f9b0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'c_vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e5488e-544e-4526-809d-8ff65a058dae",
   "metadata": {},
   "source": [
    "### Bag-of-words model(wordcounts) and Vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e4d16f-0d9c-49cf-bf4a-4797870f2ce2",
   "metadata": {},
   "source": [
    "### Logistic Regression model \n",
    "找出哪个n-gram在逻辑回归模型中表现更好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b330c3aa-8b88-42b0-96af-5944d3d4a76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal n-gram:  (1, 2)\n",
      "optimal parameter:  {'c_vectorizer__ngram_range': (1, 2)}\n",
      "optimal score:  0.9555176337108469\n",
      "classification report\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.993566</td>\n",
       "      <td>0.975632</td>\n",
       "      <td>1088.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.993321</td>\n",
       "      <td>0.956801</td>\n",
       "      <td>0.974719</td>\n",
       "      <td>1088.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.975184</td>\n",
       "      <td>0.975184</td>\n",
       "      <td>0.975184</td>\n",
       "      <td>0.975184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.975827</td>\n",
       "      <td>0.975184</td>\n",
       "      <td>0.975175</td>\n",
       "      <td>2176.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.975827</td>\n",
       "      <td>0.975184</td>\n",
       "      <td>0.975175</td>\n",
       "      <td>2176.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0.0            0.958333  0.993566  0.975632  1088.000000\n",
       "1.0            0.993321  0.956801  0.974719  1088.000000\n",
       "accuracy       0.975184  0.975184  0.975184     0.975184\n",
       "macro avg      0.975827  0.975184  0.975175  2176.000000\n",
       "weighted avg   0.975827  0.975184  0.975175  2176.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_pipeline = Pipeline([\n",
    "    ('c_vectorizer', CountVectorizer()),\n",
    "    ('lr', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "gs_lr = GridSearchCV(lr_pipeline, refit=True, cv=2, param_grid=param_grid, scoring='f1', n_jobs=-1)\n",
    "gs_lr.fit(X_train, y_train)\n",
    "\n",
    "print('optimal n-gram: ', gs_lr.best_estimator_.get_params()['c_vectorizer__ngram_range'])\n",
    "print(\"optimal parameter: \", gs_lr.best_params_)\n",
    "print(\"optimal score: \", gs_lr.best_score_)\n",
    "\n",
    "print('classification report')\n",
    "predictions = gs_lr.predict(X_test)\n",
    "report = classification_report(y_test, predictions, digits=4, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51fba86-f294-415b-93fe-5569d2c80d6d",
   "metadata": {},
   "source": [
    "### Support Vector Machine model \n",
    "找出哪个n-gram在支持向量机SVM模型中表现更好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eb23476-d17a-4b5a-b2a2-9ff0dd7d53e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal n-gram:  (1, 2)\n",
      "optimal parameter:  {'c_vectorizer__ngram_range': (1, 2)}\n",
      "optimal score:  0.9467008914841224\n",
      "classification report\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.964448</td>\n",
       "      <td>0.972426</td>\n",
       "      <td>0.968421</td>\n",
       "      <td>1088.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.972196</td>\n",
       "      <td>0.964154</td>\n",
       "      <td>0.968159</td>\n",
       "      <td>1088.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.968290</td>\n",
       "      <td>0.968290</td>\n",
       "      <td>0.968290</td>\n",
       "      <td>0.96829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.968322</td>\n",
       "      <td>0.968290</td>\n",
       "      <td>0.968290</td>\n",
       "      <td>2176.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.968322</td>\n",
       "      <td>0.968290</td>\n",
       "      <td>0.968290</td>\n",
       "      <td>2176.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0.0            0.964448  0.972426  0.968421  1088.00000\n",
       "1.0            0.972196  0.964154  0.968159  1088.00000\n",
       "accuracy       0.968290  0.968290  0.968290     0.96829\n",
       "macro avg      0.968322  0.968290  0.968290  2176.00000\n",
       "weighted avg   0.968322  0.968290  0.968290  2176.00000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_pipe = Pipeline([\n",
    "    ('c_vectorizer', CountVectorizer()),\n",
    "    ('svm', svm.SVC(max_iter=-1, random_state=42))\n",
    "])\n",
    "\n",
    "gs_svm = GridSearchCV(svm_pipe, refit=True, cv=2, param_grid=param_grid, scoring='f1', n_jobs=-1)\n",
    "gs_svm.fit(X_train, y_train)\n",
    "\n",
    "print('optimal n-gram: ', gs_svm.best_estimator_.get_params()['c_vectorizer__ngram_range'])\n",
    "print(\"optimal parameter: \", gs_svm.best_params_)\n",
    "print(\"optimal score: \", gs_svm.best_score_)\n",
    "\n",
    "print('classification report')\n",
    "predictions = gs_svm.predict(X_test)\n",
    "report = classification_report(y_test, predictions, digits=4, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f321f6-6eab-4db1-9045-1fc6c8428a0e",
   "metadata": {},
   "source": [
    "【这里要改】对比上面逻辑回归和SVM交叉验证的结果，逻辑回归的最佳性能更好，因此我们选择最佳性能更好的逻辑回归结果，它的最优参数n-gram是（1，2）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0b050c-dae0-47fa-a08d-90370811bfc6",
   "metadata": {},
   "source": [
    "# Supervised Learning Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfbccbb3-2df8-423f-a724-773de613b8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      " 4882    similar reviewer avoid place year simply veget...\n",
      "4931    happy pop major change work long table got rid...\n",
      "275     month track burger fi like iroquois brave prov...\n",
      "6592    husband dine wedding anniversary note occasion...\n",
      "7150    love reading terminal shop fish head pick hoag...\n",
      "Name: words, dtype: object\n",
      "\n",
      "y_train:\n",
      " 4882    1.0\n",
      "4931    1.0\n",
      "275     0.0\n",
      "6592    1.0\n",
      "7150    1.0\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train:\\n\", X_train.head())\n",
    "print(\"\\ny_train:\\n\", y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e4b61-8673-4a42-89fe-da92301bbb77",
   "metadata": {},
   "source": [
    "### Bag-of-words model(TF-IDF) and Vectorisation\n",
    "【摘抄】we can use TF_IDF vectorizing to find the weighted words that occur more frequently in the document that leads to creation of the bag of words model我们可以使用 TF_IDF 向量化来找到文档中出现频率更高的加权词，从而创建词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eb00f8e-2d0c-4a68-956a-f9afb9df79a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "my_stop_words = set(stopwords.words('english') + \n",
    "                    list(ENGLISH_STOP_WORDS) + \n",
    "                    ['super', 'duper', 'place'])\n",
    "exclude_stopwords = ['no','not','none']\n",
    "for word in exclude_stopwords:\n",
    "    my_stop_words.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "605a213c-cbf0-4515-be64-8a1e96f26cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(sentence):\n",
    "    # to remove any space from beginning and the end of text\n",
    "    listofwords = sentence.strip().split()\n",
    "    listof_words = []    \n",
    "    for word in listofwords:\n",
    "        if not word in my_stop_words:\n",
    "            lemm_word = WordNetLemmatizer().lemmatize(word)\n",
    "            # remove the stop words\n",
    "            for punctuation_mark in string.punctuation:\n",
    "                word = word.replace(punctuation_mark, '').lower()\n",
    "            if len(word)>0:\n",
    "                listof_words.append(word)\n",
    "    return listof_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16111c-9121-4c52-81e4-f1d1cc699eae",
   "metadata": {},
   "source": [
    "从上面cross-validation得到最优的n-gram的结果(1,2)，在这里使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0158ec29-fce4-4c92-9c6f-f361ac00bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_1 = TfidfVectorizer(min_df=100,\n",
    "                         tokenizer=my_tokenizer,\n",
    "                         stop_words=list(my_stop_words), \n",
    "                         ngram_range=(1,2)).fit(X_train)\n",
    "X_train1 = vect_1.transform(X_train)\n",
    "X_test1 = vect_1.transform(X_test)\n",
    "# the below line for future coeff\n",
    "X_train1_df = pd.DataFrame(X_train1.toarray(), columns=vect_1.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ab430c2-155b-4a73-8a83-cbdd6d5b2c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>ago</th>\n",
       "      <th>amazing</th>\n",
       "      <th>ambiance</th>\n",
       "      <th>appetizer</th>\n",
       "      <th>area</th>\n",
       "      <th>arrive</th>\n",
       "      <th>...</th>\n",
       "      <th>woman</th>\n",
       "      <th>wonderful</th>\n",
       "      <th>work</th>\n",
       "      <th>worth</th>\n",
       "      <th>wow</th>\n",
       "      <th>write</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>yelp</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.215433</td>\n",
       "      <td>0.150120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158759</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.211848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.233372</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.112143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.268046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5071</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5072</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5073</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137246</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5074</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.168603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5075</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5076 rows × 473 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      able  absolutely  actually  add       ago   amazing  ambiance  \\\n",
       "0      0.0         0.0       0.0  0.0  0.215433  0.150120       0.0   \n",
       "1      0.0         0.0       0.0  0.0  0.000000  0.000000       0.0   \n",
       "2      0.0         0.0       0.0  0.0  0.000000  0.000000       0.0   \n",
       "3      0.0         0.0       0.0  0.0  0.000000  0.000000       0.0   \n",
       "4      0.0         0.0       0.0  0.0  0.000000  0.000000       0.0   \n",
       "...    ...         ...       ...  ...       ...       ...       ...   \n",
       "5071   0.0         0.0       0.0  0.0  0.000000  0.000000       0.0   \n",
       "5072   0.0         0.0       0.0  0.0  0.000000  0.000000       0.0   \n",
       "5073   0.0         0.0       0.0  0.0  0.000000  0.137246       0.0   \n",
       "5074   0.0         0.0       0.0  0.0  0.000000  0.159428       0.0   \n",
       "5075   0.0         0.0       0.0  0.0  0.000000  0.000000       0.0   \n",
       "\n",
       "      appetizer      area  arrive  ...  woman  wonderful      work     worth  \\\n",
       "0      0.000000  0.000000     0.0  ...    0.0        0.0  0.000000  0.158759   \n",
       "1      0.000000  0.223475     0.0  ...    0.0        0.0  0.211848  0.000000   \n",
       "2      0.000000  0.000000     0.0  ...    0.0        0.0  0.054746  0.000000   \n",
       "3      0.112143  0.000000     0.0  ...    0.0        0.0  0.000000  0.000000   \n",
       "4      0.000000  0.000000     0.0  ...    0.0        0.0  0.000000  0.000000   \n",
       "...         ...       ...     ...  ...    ...        ...       ...       ...   \n",
       "5071   0.000000  0.000000     0.0  ...    0.0        0.0  0.000000  0.000000   \n",
       "5072   0.000000  0.000000     0.0  ...    0.0        0.0  0.000000  0.000000   \n",
       "5073   0.000000  0.151208     0.0  ...    0.0        0.0  0.000000  0.000000   \n",
       "5074   0.185247  0.000000     0.0  ...    0.0        0.0  0.000000  0.168603   \n",
       "5075   0.000000  0.000000     0.0  ...    0.0        0.0  0.000000  0.000000   \n",
       "\n",
       "      wow  write     wrong      year  yelp       yes  \n",
       "0     0.0    0.0  0.000000  0.172716   0.0  0.000000  \n",
       "1     0.0    0.0  0.000000  0.233372   0.0  0.000000  \n",
       "2     0.0    0.0  0.064513  0.000000   0.0  0.000000  \n",
       "3     0.0    0.0  0.000000  0.000000   0.0  0.268046  \n",
       "4     0.0    0.0  0.000000  0.000000   0.0  0.000000  \n",
       "...   ...    ...       ...       ...   ...       ...  \n",
       "5071  0.0    0.0  0.000000  0.000000   0.0  0.000000  \n",
       "5072  0.0    0.0  0.000000  0.000000   0.0  0.000000  \n",
       "5073  0.0    0.0  0.000000  0.000000   0.0  0.000000  \n",
       "5074  0.0    0.0  0.000000  0.000000   0.0  0.000000  \n",
       "5075  0.0    0.0  0.000000  0.000000   0.0  0.000000  \n",
       "\n",
       "[5076 rows x 473 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_words = pd.DataFrame(columns=vect_1.get_feature_names_out(), data=X_train1.toarray())\n",
    "new_df_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d4fde2-3888-4fb1-b6f1-d82a850a18dd",
   "metadata": {},
   "source": [
    "【可以转csv，用作推荐】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89291af6-9728-42b5-9f2e-c0703b498b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>food</td>\n",
       "      <td>307.563976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>good</td>\n",
       "      <td>284.709831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>order</td>\n",
       "      <td>222.567556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>great</td>\n",
       "      <td>218.182862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>time</td>\n",
       "      <td>189.943344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>come</td>\n",
       "      <td>188.883089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>like</td>\n",
       "      <td>188.783906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>service</td>\n",
       "      <td>182.852285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>restaurant</td>\n",
       "      <td>160.727427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>try</td>\n",
       "      <td>148.129300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>eat</td>\n",
       "      <td>134.459397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>drink</td>\n",
       "      <td>130.811222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>love</td>\n",
       "      <td>129.754425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>wait</td>\n",
       "      <td>127.891931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>chicken</td>\n",
       "      <td>127.457122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          words      counts\n",
       "150        food  307.563976\n",
       "164        good  284.709831\n",
       "280       order  222.567556\n",
       "170       great  218.182862\n",
       "417        time  189.943344\n",
       "72         come  188.883089\n",
       "223        like  188.783906\n",
       "362     service  182.852285\n",
       "335  restaurant  160.727427\n",
       "428         try  148.129300\n",
       "116         eat  134.459397\n",
       "112       drink  130.811222\n",
       "236        love  129.754425\n",
       "442        wait  127.891931\n",
       "61      chicken  127.457122"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting most repetitive words \n",
    "word_counts = np.array(np.sum(X_train1, axis=0)).reshape((-1,))\n",
    "words = np.array(vect_1.get_feature_names_out())\n",
    "words_df = pd.DataFrame({\"words\":words, \"counts\":word_counts})\n",
    "words_df.sort_values(by=\"counts\",ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb8c694-f297-43f0-b72f-26a307e64c1a",
   "metadata": {},
   "source": [
    "### Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44233678-9df2-4853-a288-a6e8990945f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training set: 0.9456264775413712\n",
      "Score on test set: 0.9223345588235294\n"
     ]
    }
   ],
   "source": [
    "# Fitting Logistic regression to the training set\n",
    "logreg = LogisticRegression(solver='lbfgs',multi_class='auto',random_state=1)\n",
    "logreg.fit(X_train1, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred_logreg = logreg.predict(X_test1)\n",
    "\n",
    "# Training score\n",
    "print(f\"Score on training set: {logreg.score(X_train1,y_train)}\")\n",
    "print(f\"Score on test set: {logreg.score(X_test1,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af480924-8b22-4000-8b49-3e84d4d3ddf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True 0</th>\n",
       "      <td>1019</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 1</th>\n",
       "      <td>100</td>\n",
       "      <td>988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Predicted 0  Predicted 1\n",
       "True 0         1019           69\n",
       "True 1          100          988"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.910634</td>\n",
       "      <td>0.936581</td>\n",
       "      <td>0.923425</td>\n",
       "      <td>1088.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.934721</td>\n",
       "      <td>0.908088</td>\n",
       "      <td>0.921212</td>\n",
       "      <td>1088.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.922335</td>\n",
       "      <td>0.922335</td>\n",
       "      <td>0.922335</td>\n",
       "      <td>0.922335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.922678</td>\n",
       "      <td>0.922335</td>\n",
       "      <td>0.922319</td>\n",
       "      <td>2176.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.922678</td>\n",
       "      <td>0.922335</td>\n",
       "      <td>0.922319</td>\n",
       "      <td>2176.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0.0            0.910634  0.936581  0.923425  1088.000000\n",
       "1.0            0.934721  0.908088  0.921212  1088.000000\n",
       "accuracy       0.922335  0.922335  0.922335     0.922335\n",
       "macro avg      0.922678  0.922335  0.922319  2176.000000\n",
       "weighted avg   0.922678  0.922335  0.922319  2176.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('confusion matrix')\n",
    "con_mat_lr = confusion_matrix(y_test, y_pred_logreg)\n",
    "df_cm_lr = pd.DataFrame(con_mat_lr, columns = ['Predicted 0','Predicted 1'], index = ['True 0','True 1'])\n",
    "display(df_cm_lr)\n",
    "print('classification report')\n",
    "report = classification_report(y_test, y_pred_logreg, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04718d4f-375b-4943-b187-2478ede737ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>delicious</th>\n",
       "      <td>5.218912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>4.798466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>4.687341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>4.410005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>definitely</th>\n",
       "      <td>4.171167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mediocre</th>\n",
       "      <td>-2.979704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointed</th>\n",
       "      <td>-3.333566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terrible</th>\n",
       "      <td>-3.609629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>-3.740723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bland</th>\n",
       "      <td>-4.047275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>473 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  coef\n",
       "delicious     5.218912\n",
       "great         4.798466\n",
       "amazing       4.687341\n",
       "good          4.410005\n",
       "definitely    4.171167\n",
       "...                ...\n",
       "mediocre     -2.979704\n",
       "disappointed -3.333566\n",
       "terrible     -3.609629\n",
       "bad          -3.740723\n",
       "bland        -4.047275\n",
       "\n",
       "[473 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the most informative words\n",
    "log_odds = logreg.coef_[0]\n",
    "coeff = pd.DataFrame(log_odds, X_train1_df.columns, columns=['coef']).sort_values(by='coef', ascending=False)\n",
    "coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd1db2e-3594-41da-88ce-d5220ed4b8d6",
   "metadata": {},
   "source": [
    "### Naive Bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1133f2f0-bbdb-4790-b44c-ccf09342356f",
   "metadata": {},
   "source": [
    "# Testing Sentiment Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ab8ab5d-be2e-45bb-8ab7-6b4b2a573206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special(text):\n",
    "    # remove the URL\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # remove mentions\n",
    "    text = re.sub(\"@[^\\s]*\", \"\", text)\n",
    "    # remove hashtags\n",
    "    text = re.sub(\"#[^\\s]*\", \"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def spacy_process(text):\n",
    "    doc = nlp(text)\n",
    "    # Lemmatization with Spacy\n",
    "    lemma_list = []\n",
    "    for token in doc:\n",
    "        lemma_list.append(token.lemma_)\n",
    "            \n",
    "    #Filter the stopwords, remove non-letters and lower case \n",
    "    #filtered_sentence =[]\n",
    "    #for word in lemma_list:\n",
    "        #lexeme = nlp.vocab[word]\n",
    "        #if lexeme.is_stop == False:\n",
    "            #filtered_sentence.append(word)\n",
    "    lower_words = []\n",
    "    for word in lemma_list:\n",
    "        filtered_aplha_char = re.sub(\"[^\\w]\" , \" \" , word)\n",
    "        filtered_single_n = re.sub(\"n\\sn\", \" \" ,filtered_aplha_char)\n",
    "        text_letters_only = re.sub(\"[^a-zA-Z]\", \" \", filtered_single_n)\n",
    "        text_words_lower = text_letters_only.lower()\n",
    "        remove_single_char = re.sub(r'\\b\\w\\b', '', text_words_lower)\n",
    "        lower_words.append(remove_single_char)\n",
    "    text_final = \" \".join(lower_words)\n",
    "    return  \" \".join(text_final.split())\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Stopwords\n",
    "    my_stop_words = set(stopwords.words('english') + \n",
    "                        list(ENGLISH_STOP_WORDS) + \n",
    "                        ['super', 'duper', 'place'])\n",
    "    exclude_stopwords = ['no','not','none']\n",
    "    for word in exclude_stopwords:\n",
    "        my_stop_words.remove(word)\n",
    "    \n",
    "    word_tokens = word_tokenize(text)\n",
    "    tokens_list = list()\n",
    "    for word in word_tokens:\n",
    "        if word.isalpha() and word not in my_stop_words:\n",
    "            tokens_list.append(word)\n",
    "    return tokens_list\n",
    "\n",
    "def clean_data(text, needed_format):\n",
    "    text = remove_special(text)\n",
    "    words_sentence = spacy_process(text)\n",
    "    tokens_list = remove_stopwords(words_sentence)\n",
    "    if needed_format == 'list':\n",
    "        return tokens_list\n",
    "    elif needed_format == 'string':\n",
    "        return words_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba72279c-8a19-40cc-8978-844638e2ea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_model(text):\n",
    "    print(\"Original review is:\\n\", text)\n",
    "    print(\"\\nCleaned review is:\\n\", clean_data(text, 'string'))\n",
    "    result = logreg.predict(vect_1.transform([clean_data(text,'string')]))\n",
    "    print(\"\\nLogistic Regression model: \", result)\n",
    "    if result == 0:\n",
    "        print(\"\\nThis review has negetive sentiment\\n\")\n",
    "    elif result == 1:\n",
    "        print(\"\\nThis review has positive sentiment\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46b1269e-42a1-4ac3-aa82-8ad88d437d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original review is:\n",
      " i like the pork burger, and the pizza is delicious, but the wait time is too long\n",
      "\n",
      "Cleaned review is:\n",
      " like the pork burger and the pizza be delicious but the wait time be too long\n",
      "\n",
      "Logistic Regression model:  [1.]\n",
      "\n",
      "This review has positive sentiment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"i like the pork burger, and the pizza is delicious, but the wait time is too long\"\n",
    "testing_model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "3cbafadc-3db7-4f1c-9a7a-a7a179116692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original review is:\n",
      "  people that works here for sure is friendly! :)I do love that big menu book and seems like there are a lot of items to choose from. This is always nice as Vietnamese food is definitely more than pho and more spring rolls.\n",
      "\n",
      "Cleaned review is:\n",
      " people that work here for sure be friendly do love that big menu book and seem like there be lot of item to choose from this be always nice as vietnamese food be definitely more than pho and more spring roll\n",
      "\n",
      "Logistic Regression model:  [1.]\n",
      "\n",
      "This review has positive sentiment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \" people that works here for sure is friendly! :)I do love that big menu book and seems like there are a lot of items to choose from. This is always nice as Vietnamese food is definitely more than pho and more spring rolls.\"\n",
    "testing_model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3ac829-f59c-4988-9cf7-6720c80ac5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
