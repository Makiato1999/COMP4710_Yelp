{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11a9669e-c92d-45ae-81b7-b59e2e8523af",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The previous stage includes reviews text preprocessing, which is in Preprocess_Reviews.ipynb<br>\n",
    "The following stage is aim to analysis reviews sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a9dbb51f-8804-4366-87b7-f9a3bc567865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "import seaborn as sns\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import string\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2882f14-c15d-49e7-844b-49aa4a14dab9",
   "metadata": {},
   "source": [
    "# Load reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "788b8514-5d9e-442a-b96c-11e5d8bf35ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gt_h5A-A5Nt50EbaPCorvw</td>\n",
       "      <td>F8yozE3NWnImNApHO347gQ</td>\n",
       "      <td>First impression was good but their food is ho...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>impression good food horrible cash idea over p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0dELc-isYD5Av7KNsIvcRA</td>\n",
       "      <td>WxB8498ejPtHE7wFa89_fA</td>\n",
       "      <td>I've been to this location countless times, an...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>location countless time time food service ambi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bmrq4dvmlVMGU3roXzeDgQ</td>\n",
       "      <td>-mIlmp5l4hKlp1tvHRdvTg</td>\n",
       "      <td>Seems popular- but not that delish. Short sub ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>popular delish short sub maybe typical length ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tCP2EjtzGJ7JEHlj_8i1xw</td>\n",
       "      <td>hcxea89M_U__LADtu3C0kA</td>\n",
       "      <td>The service was great! The place was very beau...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>service great place beautiful small tight rest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3gM_kcsqfU9eqmE19kL4tw</td>\n",
       "      <td>3BJxm-HnvzdwD1zjmSbmyQ</td>\n",
       "      <td>I was at this restaurant recently and was so u...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>restaurant recently unhappy purchase wonton so...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id             business_id  \\\n",
       "0  Gt_h5A-A5Nt50EbaPCorvw  F8yozE3NWnImNApHO347gQ   \n",
       "1  0dELc-isYD5Av7KNsIvcRA  WxB8498ejPtHE7wFa89_fA   \n",
       "2  Bmrq4dvmlVMGU3roXzeDgQ  -mIlmp5l4hKlp1tvHRdvTg   \n",
       "3  tCP2EjtzGJ7JEHlj_8i1xw  hcxea89M_U__LADtu3C0kA   \n",
       "4  3gM_kcsqfU9eqmE19kL4tw  3BJxm-HnvzdwD1zjmSbmyQ   \n",
       "\n",
       "                                                text  target  \\\n",
       "0  First impression was good but their food is ho...     0.0   \n",
       "1  I've been to this location countless times, an...     0.0   \n",
       "2  Seems popular- but not that delish. Short sub ...     0.0   \n",
       "3  The service was great! The place was very beau...     0.0   \n",
       "4  I was at this restaurant recently and was so u...     0.0   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  impression good food horrible cash idea over p...  \n",
       "1  location countless time time food service ambi...  \n",
       "2  popular delish short sub maybe typical length ...  \n",
       "3  service great place beautiful small tight rest...  \n",
       "4  restaurant recently unhappy purchase wonton so...  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"Cleaned_Text_Dataset.csv\"\n",
    "df = pd.read_csv(file)\n",
    "del df[\"Unnamed: 0\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc15cc1b-5533-4b41-839a-d9af67736bfb",
   "metadata": {},
   "source": [
    "# Tokenization and Bag-of-Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92c20ac-dca4-4eae-a704-da60e3813440",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['cleaned_text']\n",
    "word_tokens = word_tokenize(text)\n",
    "tokens = list()\n",
    "for word in word_tokens:\n",
    "    if word.isalpha() and word not in my_stop_words:\n",
    "        tokens.append(word)\n",
    "token_dist = FreqDist(tokens)\n",
    "dist = pd.DataFrame(token_dist.most_common(20),columns=['Word', 'Frequency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfb9b82-d522-49ba-b800-cdbfca32dff1",
   "metadata": {},
   "source": [
    "为了方便接下来的使用，我需要将dataframe里cleaned_text转为token list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "97ae5e5e-4357-4841-8694-5b79b588f2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef list_of_token(text):\\n    return [w for w in text.split()]\\ndf['cleaned_text'] = df['cleaned_text'].apply(lambda text: list_of_token(text))\\ndf\\n\""
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def list_of_token(text):\n",
    "    return [w for w in text.split()]\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda text: list_of_token(text))\n",
    "df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "cfbccbb3-2df8-423f-a724-773de613b8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4882    similar reviewer avoid place year simply veget...\n",
       "4931    happy pop major change work more long table go...\n",
       "275     month track burger fi like iroquois brave prov...\n",
       "6592    husband dine wedding anniversary note occasion...\n",
       "7150    love reading terminal shop fish head pick hoag...\n",
       "Name: cleaned_text, dtype: object"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df['cleaned_text'] \n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify=y)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3c6536a8-5be4-405c-ae3a-bd63a6a15556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4882    1.0\n",
       "4931    1.0\n",
       "275     0.0\n",
       "6592    1.0\n",
       "7150    1.0\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec2e8d-7cee-4ecb-8f1e-aa8e32dd525f",
   "metadata": {},
   "source": [
    "建立评估function，它可以生成confusion matrix的heatmap，直观"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8484cf45-a611-4f3d-bb1a-b6570426df60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_test, predictions):\n",
    "    cf_matrix = confusion_matrix(y_test, predictions)\n",
    "    sns.heatmap(cf_matrix, annot = True, fmt = 'd',cmap=\"Blues\")\n",
    "    plt.title('Heatmap of confusion matrix for Test data')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57144e06-660d-4d15-93cd-4e1f03a913ff",
   "metadata": {},
   "source": [
    "# Vectorisation and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2612c617-7bae-431e-a33a-9d74c11d422a",
   "metadata": {},
   "source": [
    "列举我们将要实验的n-gram，【摘抄】GridSearchCV是Sklearn model_selection包的一个模块，用于超参数调整。 给定一组不同的超参数，GridSearchCV 循环浏览所有可能的超参数值和组合，并在训练数据集上拟合模型。 在这个过程中，它能够确定产生最佳精度的超参数的最佳值和组合（从给定的参数集中）【摘抄】在机器学习模型中，需要人工选择的参数称为超参数。比如随机森林中决策树的个数，人工神经网络模型中隐藏层层数和每层的节点个数，正则项中常数大小等等，他们都需要事先指定。超参数选择不恰当，就会出现欠拟合或者过拟合的问题。而在选择超参数的时候，有两个途径，一个是凭经验微调，另一个就是选择不同大小的参数，带入模型中，挑选表现最好的参数。微调的一种方法是手工调制超参数，直到找到一个好的超参数组合，这么做的话会非常冗长，你也可能没有时间探索多种组合，所以可以使用Scikit-Learn的GridSearchCV来做这项搜索工作。<br>\n",
    "这里用的是后者<br>\n",
    "可以提一下交叉验证，cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c36b98f5-3fb0-41b4-9e60-412497111302",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'c_vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6461e82c-57cd-4fd1-8204-e9093b9343c0",
   "metadata": {},
   "source": [
    "### Logistic Regression model \n",
    "找出哪个n-gram在逻辑回归模型中表现更好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a39cee3d-502e-4a4d-9928-c536986759ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal n-gram: (1, 2)\n",
      "最优参数:  {'c_vectorizer__ngram_range': (1, 2)}\n",
      "最佳性能:  0.9572187853968808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9591    0.9917    0.9751      1088\n",
      "         1.0     0.9914    0.9577    0.9743      1088\n",
      "\n",
      "    accuracy                         0.9747      2176\n",
      "   macro avg     0.9753    0.9747    0.9747      2176\n",
      "weighted avg     0.9753    0.9747    0.9747      2176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_pipeline = Pipeline([\n",
    "    ('c_vectorizer', CountVectorizer()),\n",
    "    ('lr', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "gs_lr = GridSearchCV(lr_pipeline, refit=True, cv=2, param_grid=param_grid, scoring='f1', n_jobs=-1)\n",
    "gs_lr.fit(X_train, y_train)\n",
    "\n",
    "print('Optimal n-gram:', gs_lr.best_estimator_.get_params()['c_vectorizer__ngram_range'])\n",
    "print(\"最优参数: \", gs_lr.best_params_)\n",
    "print(\"最佳性能: \", gs_lr.best_score_)\n",
    "\n",
    "predictions = gs_lr.predict(X_test)\n",
    "print(classification_report(y_test, predictions, digits=4))\n",
    "#evaluate(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e9f1a3-3a51-468c-b4f8-3277594305fe",
   "metadata": {},
   "source": [
    "### Support Vector Machine model \n",
    "找出哪个n-gram在支持向量机SVM模型中表现更好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5bfc5448-a7d3-472d-a91f-be1481f6ff78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ngram_range: (1, 2)\n",
      "最优参数:  {'c_vectorizer__ngram_range': (1, 2)}\n",
      "最佳性能:  0.9494188226471691\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9662    0.9724    0.9693      1088\n",
      "         1.0     0.9722    0.9660    0.9691      1088\n",
      "\n",
      "    accuracy                         0.9692      2176\n",
      "   macro avg     0.9692    0.9692    0.9692      2176\n",
      "weighted avg     0.9692    0.9692    0.9692      2176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_pipe = Pipeline([\n",
    "    ('c_vectorizer', CountVectorizer()),\n",
    "    ('svm', svm.SVC(max_iter=-1, random_state=42))\n",
    "])\n",
    "\n",
    "gs_svm = GridSearchCV(svm_pipe, refit=True, cv=2, param_grid=param_grid, scoring='f1', n_jobs=-1)\n",
    "gs_svm.fit(X_train, y_train)\n",
    "\n",
    "print('Best ngram_range:', gs_svm.best_estimator_.get_params()['c_vectorizer__ngram_range'])\n",
    "print(\"最优参数: \", gs_svm.best_params_)\n",
    "print(\"最佳性能: \", gs_svm.best_score_)\n",
    "\n",
    "predictions = gs_svm.predict(X_test)\n",
    "print(classification_report(y_test, predictions, digits=4))\n",
    "#evaluate(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3816dfd3-2c4f-4dc9-9c6d-eaf2cbbdbaf7",
   "metadata": {},
   "source": [
    "对比上面逻辑回归和SVM交叉验证的结果，逻辑回归的最佳性能更好，因此我们选择最佳性能更好的逻辑回归结果，它的最优参数n-gram是（1，2）<br>\n",
    "接下来使用逻辑回归和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6c01a5-d31e-4a23-b922-3c13395ae0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\"lr__C\":[1, 10, 20, 30, 40], 'lr__l1_ratio':['none', 0, 0.1], 'lr__penalty':['l2', 'elasticnet'],\n",
    "       'lr__solver':['lbfgs', 'saga']}\n",
    "\n",
    "# Pipeline for TfidfVectorizer - with the best ngram_range - and Logistic Regression Classifier \n",
    "lr_pipe_tfidf_2 = Pipeline([\n",
    " ('tfidf', TfidfVectorizer(ngram_range = (1,2))),\n",
    " ('lr', LogisticRegression(max_iter=1500, n_jobs = -1, random_state=42))\n",
    "])\n",
    "\n",
    "logreg_cv_3=GridSearchCV(lr_pipe_tfidf_2, grid, scoring='f1', cv=2)\n",
    "logreg_cv_3.fit(X_train, y_train)\n",
    "\n",
    "print('Best l1_ratio:', logreg_cv_3.best_estimator_.get_params()['lr__l1_ratio'])\n",
    "print('Best C:', logreg_cv_3.best_estimator_.get_params()['lr__C'])\n",
    "print('Best penalty:', logreg_cv_3.best_estimator_.get_params()['lr__penalty'])\n",
    "print('Best solver:', logreg_cv_3.best_estimator_.get_params()['lr__solver'])\n",
    "\n",
    "predictions = logreg_cv_3.predict(X_test)\n",
    "evaluate(y_test, predictions )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a564ba-aae2-465d-a802-c0179a2feece",
   "metadata": {},
   "source": [
    "### Using TF_IDF\n",
    "【摘抄】we can use TF_IDF vectorizing to find the weighted words that occur more frequently in the document that leads to creation of the bag of words model我们可以使用 TF_IDF 向量化来找到文档中出现频率更高的加权词，从而创建词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3708bb-139c-4a17-8460-20013f3a9252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef98d02a-d089-4fd1-8ae2-e8af51de1dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
