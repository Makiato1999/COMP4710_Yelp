{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11a9669e-c92d-45ae-81b7-b59e2e8523af",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The previous stage includes reviews text preprocessing, which is in Process_Reviews.ipynb<br>\n",
    "The following stage is aim to analysis reviews sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9dbb51f-8804-4366-87b7-f9a3bc567865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\AA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\AA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\AA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
    "import seaborn as sns\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from gensim.models import Word2Vec\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import string\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2882f14-c15d-49e7-844b-49aa4a14dab9",
   "metadata": {},
   "source": [
    "# Load reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "788b8514-5d9e-442a-b96c-11e5d8bf35ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>review_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fxWnU4OqONBNoQhEcyazSg</td>\n",
       "      <td>krTHKI0YOpASr4gz2CVWFw</td>\n",
       "      <td>This location used to be good, several years a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>this location use to be good several year ago ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>FhtER9SGsEYkEhRcs09rsQ</td>\n",
       "      <td>krTHKI0YOpASr4gz2CVWFw</td>\n",
       "      <td>I love Cosi but this Cosi is going down hill f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>love cosi but this cosi be go down hill fast a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0KlwfaHZyvao41_3S47dyg</td>\n",
       "      <td>w9hS5x1F52Id-G1KTrAOZg</td>\n",
       "      <td>Was not a fan of their cheesesteak. Their wiz ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>be not fan of their cheesesteak their wiz sauc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2qeje7dttkvREbccHev6Pg</td>\n",
       "      <td>7lwe7n-Yc-V9E_HfLAeylg</td>\n",
       "      <td>It pains me to write this, but I fear I must.....</td>\n",
       "      <td>0.0</td>\n",
       "      <td>it pain to write this but fear must use to rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1OR23O0giNcxNbFAi4jgcg</td>\n",
       "      <td>DsKzHnkLKnxZTVsFpts4oA</td>\n",
       "      <td>Cocktails were nice however the bartender Paul...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cocktail be nice however the bartender paul be...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.2  Unnamed: 0.1               review_id             business_id  \\\n",
       "0             0             0  fxWnU4OqONBNoQhEcyazSg  krTHKI0YOpASr4gz2CVWFw   \n",
       "1             1             1  FhtER9SGsEYkEhRcs09rsQ  krTHKI0YOpASr4gz2CVWFw   \n",
       "2             2             2  0KlwfaHZyvao41_3S47dyg  w9hS5x1F52Id-G1KTrAOZg   \n",
       "3             3             3  2qeje7dttkvREbccHev6Pg  7lwe7n-Yc-V9E_HfLAeylg   \n",
       "4             4             4  1OR23O0giNcxNbFAi4jgcg  DsKzHnkLKnxZTVsFpts4oA   \n",
       "\n",
       "                                                text  target  \\\n",
       "0  This location used to be good, several years a...     0.0   \n",
       "1  I love Cosi but this Cosi is going down hill f...     0.0   \n",
       "2  Was not a fan of their cheesesteak. Their wiz ...     0.0   \n",
       "3  It pains me to write this, but I fear I must.....     0.0   \n",
       "4  Cocktails were nice however the bartender Paul...     0.0   \n",
       "\n",
       "                                               words  \n",
       "0  this location use to be good several year ago ...  \n",
       "1  love cosi but this cosi be go down hill fast a...  \n",
       "2  be not fan of their cheesesteak their wiz sauc...  \n",
       "3  it pain to write this but fear must use to rea...  \n",
       "4  cocktail be nice however the bartender paul be...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"Cleaned_Text_Dataset.csv\"\n",
    "df = pd.read_csv(file)\n",
    "del df[\"Unnamed: 0\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078c8fdd-8a0d-4442-a015-8af277852562",
   "metadata": {},
   "source": [
    "# Sentiment polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2cd247-9cb6-46e4-b248-173d6783995a",
   "metadata": {},
   "source": [
    "我们将情绪分为四个类别，negative、neutral、positive and compound。【摘抄】The first three are easy to understand and for the compound score, it is a combination of positive and negative scores and ranges from -1 to 1: below 0 is negative and above 0 is positive. I am going to use the compound score to measure the sentiment.前三个很容易理解，对于复合分数，它是正分数和负分数的组合，范围从 -1 到 1：低于 0 为负，高于 0 为正。我将使用复合分数来衡量情绪。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34ee8ad8-2b95-4007-88e3-16e46451104c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this location use to be good several year ago about year ago it start to go downhill and now it be just terrible there be people work on saturday evening nearly all the table be full and there be people wait to order and to pay the sandwich that be make for be on hard bread burn and barely edible they need to either shape up or ship out\n",
      "0    {'neg': 0.08, 'neu': 0.881, 'pos': 0.039, 'com...\n",
      "1    {'neg': 0.253, 'neu': 0.702, 'pos': 0.046, 'co...\n",
      "2    {'neg': 0.11, 'neu': 0.813, 'pos': 0.077, 'com...\n",
      "3    {'neg': 0.22, 'neu': 0.62, 'pos': 0.16, 'compo...\n",
      "4    {'neg': 0.113, 'neu': 0.734, 'pos': 0.153, 'co...\n",
      "Name: words, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Instantiate new SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "# Generate sentiment scores\n",
    "sentiment_scores = df['words'].apply(sid.polarity_scores)\n",
    "sentiment = sentiment_scores.apply(lambda x: x['compound'])\n",
    "print(df['words'][0])\n",
    "print(sentiment_scores.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de3e6d1-391c-4aca-8288-f666f35d1170",
   "metadata": {},
   "source": [
    "# Split training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "787f4fcf-c4b2-4ca8-9038-f0e62d17d953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      " 1833    have read the review that this remind someone ...\n",
      "3046    sit by the huge window you have somewhat of ni...\n",
      "5958    be generous here and should be but have nice t...\n",
      "8688    excellent place to have conversation and enjoy...\n",
      "2672    let set scene for you it be thursday and my bi...\n",
      "Name: words, dtype: object\n",
      "\n",
      "y_train:\n",
      " 1833    0.0\n",
      "3046    0.0\n",
      "5958    1.0\n",
      "8688    1.0\n",
      "2672    0.0\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X = df['words'] \n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify=y)\n",
    "print(\"X_train:\\n\", X_train.head())\n",
    "print(\"\\ny_train:\\n\", y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ff9c5-114e-4eaa-be38-904bbd8772b0",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fafc8d5-64e6-41dd-85e8-50609b72edbd",
   "metadata": {},
   "source": [
    "列举我们将要实验的n-gram，【摘抄】GridSearchCV是Sklearn model_selection包的一个模块，用于超参数调整。 给定一组不同的超参数，GridSearchCV 循环浏览所有可能的超参数值和组合，并在训练数据集上拟合模型。 在这个过程中，它能够确定产生最佳精度的超参数的最佳值和组合（从给定的参数集中）【摘抄】在机器学习模型中，需要人工选择的参数称为超参数。比如随机森林中决策树的个数，人工神经网络模型中隐藏层层数和每层的节点个数，正则项中常数大小等等，他们都需要事先指定。超参数选择不恰当，就会出现欠拟合或者过拟合的问题。而在选择超参数的时候，有两个途径，一个是凭经验微调，另一个就是选择不同大小的参数，带入模型中，挑选表现最好的参数。微调的一种方法是手工调制超参数，直到找到一个好的超参数组合，这么做的话会非常冗长，你也可能没有时间探索多种组合，所以可以使用Scikit-Learn的GridSearchCV来做这项搜索工作。<br>\n",
    "这里用的是后者<br>\n",
    "可以提一下交叉验证，cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fff587b-48fd-4467-9fc2-93b96f9b0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'c_vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e5488e-544e-4526-809d-8ff65a058dae",
   "metadata": {},
   "source": [
    "### Bag-of-words model(wordcounts) and Vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e4d16f-0d9c-49cf-bf4a-4797870f2ce2",
   "metadata": {},
   "source": [
    "### Logistic Regression model \n",
    "找出哪个n-gram在逻辑回归模型中表现更好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b330c3aa-8b88-42b0-96af-5944d3d4a76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal n-gram:  (1, 2)\n",
      "optimal parameter:  {'c_vectorizer__ngram_range': (1, 2)}\n",
      "optimal score:  0.9169781653216704\n",
      "classification report\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.928313</td>\n",
       "      <td>0.935766</td>\n",
       "      <td>0.932025</td>\n",
       "      <td>1370.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.935199</td>\n",
       "      <td>0.927684</td>\n",
       "      <td>0.931426</td>\n",
       "      <td>1369.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.931727</td>\n",
       "      <td>0.931727</td>\n",
       "      <td>0.931727</td>\n",
       "      <td>0.931727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.931756</td>\n",
       "      <td>0.931725</td>\n",
       "      <td>0.931726</td>\n",
       "      <td>2739.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.931755</td>\n",
       "      <td>0.931727</td>\n",
       "      <td>0.931726</td>\n",
       "      <td>2739.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0.0            0.928313  0.935766  0.932025  1370.000000\n",
       "1.0            0.935199  0.927684  0.931426  1369.000000\n",
       "accuracy       0.931727  0.931727  0.931727     0.931727\n",
       "macro avg      0.931756  0.931725  0.931726  2739.000000\n",
       "weighted avg   0.931755  0.931727  0.931726  2739.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_pipeline = Pipeline([\n",
    "    ('c_vectorizer', CountVectorizer()),\n",
    "    ('lr', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "gs_lr = GridSearchCV(lr_pipeline, refit=True, cv=2, param_grid=param_grid, scoring='f1', n_jobs=-1)\n",
    "gs_lr.fit(X_train, y_train)\n",
    "\n",
    "print('optimal n-gram: ', gs_lr.best_estimator_.get_params()['c_vectorizer__ngram_range'])\n",
    "print(\"optimal parameter: \", gs_lr.best_params_)\n",
    "print(\"optimal score: \", gs_lr.best_score_)\n",
    "\n",
    "print('classification report')\n",
    "predictions = gs_lr.predict(X_test)\n",
    "report = classification_report(y_test, predictions, digits=4, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51fba86-f294-415b-93fe-5569d2c80d6d",
   "metadata": {},
   "source": [
    "### Support Vector Machine model \n",
    "找出哪个n-gram在支持向量机SVM模型中表现更好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eb23476-d17a-4b5a-b2a2-9ff0dd7d53e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal n-gram:  (1, 2)\n",
      "optimal parameter:  {'c_vectorizer__ngram_range': (1, 2)}\n",
      "optimal score:  0.9467008914841224\n",
      "classification report\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.964448</td>\n",
       "      <td>0.972426</td>\n",
       "      <td>0.968421</td>\n",
       "      <td>1088.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.972196</td>\n",
       "      <td>0.964154</td>\n",
       "      <td>0.968159</td>\n",
       "      <td>1088.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.968290</td>\n",
       "      <td>0.968290</td>\n",
       "      <td>0.968290</td>\n",
       "      <td>0.96829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.968322</td>\n",
       "      <td>0.968290</td>\n",
       "      <td>0.968290</td>\n",
       "      <td>2176.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.968322</td>\n",
       "      <td>0.968290</td>\n",
       "      <td>0.968290</td>\n",
       "      <td>2176.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0.0            0.964448  0.972426  0.968421  1088.00000\n",
       "1.0            0.972196  0.964154  0.968159  1088.00000\n",
       "accuracy       0.968290  0.968290  0.968290     0.96829\n",
       "macro avg      0.968322  0.968290  0.968290  2176.00000\n",
       "weighted avg   0.968322  0.968290  0.968290  2176.00000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_pipe = Pipeline([\n",
    "    ('c_vectorizer', CountVectorizer()),\n",
    "    ('svm', svm.SVC(max_iter=-1, random_state=42))\n",
    "])\n",
    "\n",
    "gs_svm = GridSearchCV(svm_pipe, refit=True, cv=2, param_grid=param_grid, scoring='f1', n_jobs=-1)\n",
    "gs_svm.fit(X_train, y_train)\n",
    "\n",
    "print('optimal n-gram: ', gs_svm.best_estimator_.get_params()['c_vectorizer__ngram_range'])\n",
    "print(\"optimal parameter: \", gs_svm.best_params_)\n",
    "print(\"optimal score: \", gs_svm.best_score_)\n",
    "\n",
    "print('classification report')\n",
    "predictions = gs_svm.predict(X_test)\n",
    "report = classification_report(y_test, predictions, digits=4, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f321f6-6eab-4db1-9045-1fc6c8428a0e",
   "metadata": {},
   "source": [
    "【这里要改】对比上面逻辑回归和SVM交叉验证的结果，逻辑回归的最佳性能更好，因此我们选择最佳性能更好的逻辑回归结果，它的最优参数n-gram是（1，2）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0b050c-dae0-47fa-a08d-90370811bfc6",
   "metadata": {},
   "source": [
    "# Supervised Learning Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfbccbb3-2df8-423f-a724-773de613b8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      " 4882    similar reviewer avoid place year simply veget...\n",
      "4931    happy pop major change work long table got rid...\n",
      "275     month track burger fi like iroquois brave prov...\n",
      "6592    husband dine wedding anniversary note occasion...\n",
      "7150    love reading terminal shop fish head pick hoag...\n",
      "Name: words, dtype: object\n",
      "\n",
      "y_train:\n",
      " 4882    1.0\n",
      "4931    1.0\n",
      "275     0.0\n",
      "6592    1.0\n",
      "7150    1.0\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train:\\n\", X_train.head())\n",
    "print(\"\\ny_train:\\n\", y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e4b61-8673-4a42-89fe-da92301bbb77",
   "metadata": {},
   "source": [
    "### Bag-of-words model(TF-IDF) and Vectorisation\n",
    "【摘抄】we can use TF_IDF vectorizing to find the weighted words that occur more frequently in the document that leads to creation of the bag of words model我们可以使用 TF_IDF 向量化来找到文档中出现频率更高的加权词，从而创建词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6eb00f8e-2d0c-4a68-956a-f9afb9df79a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "my_stop_words = set(stopwords.words('english') + \n",
    "                    list(ENGLISH_STOP_WORDS) + \n",
    "                    ['super', 'duper', 'place'])\n",
    "exclude_stopwords = ['no','none']\n",
    "for word in exclude_stopwords:\n",
    "    my_stop_words.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "605a213c-cbf0-4515-be64-8a1e96f26cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(sentence):\n",
    "    # to remove any space from beginning and the end of text\n",
    "    listofwords = sentence.strip().split()\n",
    "    listof_words = []    \n",
    "    for word in listofwords:\n",
    "        if not word in my_stop_words:\n",
    "            lemm_word = WordNetLemmatizer().lemmatize(word)\n",
    "            # remove the stop words\n",
    "            for punctuation_mark in string.punctuation:\n",
    "                word = word.replace(punctuation_mark, '').lower()\n",
    "            if len(word)>0:\n",
    "                listof_words.append(word)\n",
    "    return listof_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16111c-9121-4c52-81e4-f1d1cc699eae",
   "metadata": {},
   "source": [
    "从上面cross-validation得到最优的n-gram的结果(1,2)，在这里使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0158ec29-fce4-4c92-9c6f-f361ac00bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_1 = TfidfVectorizer(min_df=100,\n",
    "                         tokenizer=my_tokenizer,\n",
    "                         stop_words=list(my_stop_words), \n",
    "                         ngram_range=(1,2)).fit(X_train)\n",
    "X_train1 = vect_1.transform(X_train)\n",
    "X_test1 = vect_1.transform(X_test)\n",
    "# the below line for future coeff\n",
    "X_train1_df = pd.DataFrame(X_train1.toarray(), columns=vect_1.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ab430c2-155b-4a73-8a83-cbdd6d5b2c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>ago</th>\n",
       "      <th>amazing</th>\n",
       "      <th>ambiance</th>\n",
       "      <th>apologize</th>\n",
       "      <th>app</th>\n",
       "      <th>appetizer</th>\n",
       "      <th>...</th>\n",
       "      <th>world</th>\n",
       "      <th>worth</th>\n",
       "      <th>wow</th>\n",
       "      <th>wrap</th>\n",
       "      <th>write</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>yelp</th>\n",
       "      <th>yes</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.127336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.147377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6386</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.27462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6387</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.140117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6388</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.346725</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6389</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6390</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6391 rows × 574 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      able  absolutely  actually  add  ago  amazing  ambiance  apologize  app  \\\n",
       "0      0.0         0.0  0.127336  0.0  0.0      0.0       0.0        0.0  0.0   \n",
       "1      0.0         0.0  0.000000  0.0  0.0      0.0       0.0        0.0  0.0   \n",
       "2      0.0         0.0  0.000000  0.0  0.0      0.0       0.0        0.0  0.0   \n",
       "3      0.0         0.0  0.000000  0.0  0.0      0.0       0.0        0.0  0.0   \n",
       "4      0.0         0.0  0.000000  0.0  0.0      0.0       0.0        0.0  0.0   \n",
       "...    ...         ...       ...  ...  ...      ...       ...        ...  ...   \n",
       "6386   0.0         0.0  0.000000  0.0  0.0      0.0       0.0        0.0  0.0   \n",
       "6387   0.0         0.0  0.000000  0.0  0.0      0.0       0.0        0.0  0.0   \n",
       "6388   0.0         0.0  0.000000  0.0  0.0      0.0       0.0        0.0  0.0   \n",
       "6389   0.0         0.0  0.000000  0.0  0.0      0.0       0.0        0.0  0.0   \n",
       "6390   0.0         0.0  0.000000  0.0  0.0      0.0       0.0        0.0  0.0   \n",
       "\n",
       "      appetizer  ...     world     worth  wow     wrap  write    wrong  year  \\\n",
       "0      0.000000  ...  0.000000  0.116448  0.0  0.00000    0.0  0.12835   0.0   \n",
       "1      0.000000  ...  0.000000  0.000000  0.0  0.00000    0.0  0.00000   0.0   \n",
       "2      0.147377  ...  0.000000  0.000000  0.0  0.00000    0.0  0.00000   0.0   \n",
       "3      0.000000  ...  0.000000  0.000000  0.0  0.00000    0.0  0.00000   0.0   \n",
       "4      0.000000  ...  0.138685  0.000000  0.0  0.00000    0.0  0.00000   0.0   \n",
       "...         ...  ...       ...       ...  ...      ...    ...      ...   ...   \n",
       "6386   0.000000  ...  0.000000  0.098980  0.0  0.27462    0.0  0.00000   0.0   \n",
       "6387   0.000000  ...  0.000000  0.140117  0.0  0.00000    0.0  0.00000   0.0   \n",
       "6388   0.000000  ...  0.000000  0.346725  0.0  0.00000    0.0  0.00000   0.0   \n",
       "6389   0.000000  ...  0.000000  0.000000  0.0  0.00000    0.0  0.00000   0.0   \n",
       "6390   0.000000  ...  0.000000  0.000000  0.0  0.00000    0.0  0.00000   0.0   \n",
       "\n",
       "      yelp  yes  yummy  \n",
       "0      0.0  0.0    0.0  \n",
       "1      0.0  0.0    0.0  \n",
       "2      0.0  0.0    0.0  \n",
       "3      0.0  0.0    0.0  \n",
       "4      0.0  0.0    0.0  \n",
       "...    ...  ...    ...  \n",
       "6386   0.0  0.0    0.0  \n",
       "6387   0.0  0.0    0.0  \n",
       "6388   0.0  0.0    0.0  \n",
       "6389   0.0  0.0    0.0  \n",
       "6390   0.0  0.0    0.0  \n",
       "\n",
       "[6391 rows x 574 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_words = pd.DataFrame(columns=vect_1.get_feature_names_out(), data=X_train1.toarray())\n",
    "new_df_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d4fde2-3888-4fb1-b6f1-d82a850a18dd",
   "metadata": {},
   "source": [
    "【可以转csv，用作推荐】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89291af6-9728-42b5-9f2e-c0703b498b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>food</td>\n",
       "      <td>362.652794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>good</td>\n",
       "      <td>331.345566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>order</td>\n",
       "      <td>279.012856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>great</td>\n",
       "      <td>255.947928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>come</td>\n",
       "      <td>229.544296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>time</td>\n",
       "      <td>229.380439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>service</td>\n",
       "      <td>221.999967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>like</td>\n",
       "      <td>216.044087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>make</td>\n",
       "      <td>183.868149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>restaurant</td>\n",
       "      <td>177.163990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>try</td>\n",
       "      <td>174.384691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>really</td>\n",
       "      <td>168.240804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>no</td>\n",
       "      <td>163.641771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>say</td>\n",
       "      <td>161.314815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>drink</td>\n",
       "      <td>158.357973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          words      counts\n",
       "178        food  362.652794\n",
       "199        good  331.345566\n",
       "344       order  279.012856\n",
       "204       great  255.947928\n",
       "86         come  229.544296\n",
       "509        time  229.380439\n",
       "442     service  221.999967\n",
       "272        like  216.044087\n",
       "293        make  183.868149\n",
       "409  restaurant  177.163990\n",
       "522         try  174.384691\n",
       "397      really  168.240804\n",
       "331          no  163.641771\n",
       "431         say  161.314815\n",
       "136       drink  158.357973"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting most repetitive words \n",
    "word_counts = np.array(np.sum(X_train1, axis=0)).reshape((-1,))\n",
    "words = np.array(vect_1.get_feature_names_out())\n",
    "words_df = pd.DataFrame({\"words\":words, \"counts\":word_counts})\n",
    "words_df.sort_values(by=\"counts\",ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb8c694-f297-43f0-b72f-26a307e64c1a",
   "metadata": {},
   "source": [
    "### Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44233678-9df2-4853-a288-a6e8990945f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training set: 0.9216085119699577\n",
      "Score on test set: 0.9025191675794085\n"
     ]
    }
   ],
   "source": [
    "# Fitting Logistic regression to the training set\n",
    "logreg = LogisticRegression(solver='lbfgs',multi_class='auto',random_state=1)\n",
    "logreg.fit(X_train1, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred_logreg = logreg.predict(X_test1)\n",
    "\n",
    "# Training score\n",
    "print(f\"Score on training set: {logreg.score(X_train1,y_train)}\")\n",
    "print(f\"Score on test set: {logreg.score(X_test1,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af480924-8b22-4000-8b49-3e84d4d3ddf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True 0</th>\n",
       "      <td>1238</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 1</th>\n",
       "      <td>135</td>\n",
       "      <td>1234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Predicted 0  Predicted 1\n",
       "True 0         1238          132\n",
       "True 1          135         1234"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.901675</td>\n",
       "      <td>0.903650</td>\n",
       "      <td>0.902661</td>\n",
       "      <td>1370.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.903367</td>\n",
       "      <td>0.901388</td>\n",
       "      <td>0.902377</td>\n",
       "      <td>1369.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.902519</td>\n",
       "      <td>0.902519</td>\n",
       "      <td>0.902519</td>\n",
       "      <td>0.902519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.902521</td>\n",
       "      <td>0.902519</td>\n",
       "      <td>0.902519</td>\n",
       "      <td>2739.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.902521</td>\n",
       "      <td>0.902519</td>\n",
       "      <td>0.902519</td>\n",
       "      <td>2739.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0.0            0.901675  0.903650  0.902661  1370.000000\n",
       "1.0            0.903367  0.901388  0.902377  1369.000000\n",
       "accuracy       0.902519  0.902519  0.902519     0.902519\n",
       "macro avg      0.902521  0.902519  0.902519  2739.000000\n",
       "weighted avg   0.902521  0.902519  0.902519  2739.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('confusion matrix')\n",
    "con_mat_lr = confusion_matrix(y_test, y_pred_logreg)\n",
    "df_cm_lr = pd.DataFrame(con_mat_lr, columns = ['Predicted 0','Predicted 1'], index = ['True 0','True 1'])\n",
    "display(df_cm_lr)\n",
    "print('classification report')\n",
    "report = classification_report(y_test, y_pred_logreg, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04718d4f-375b-4943-b187-2478ede737ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>delicious</th>\n",
       "      <td>5.730341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>5.246080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>5.080927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>4.471123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awesome</th>\n",
       "      <td>3.967118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mediocre</th>\n",
       "      <td>-3.638701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rude</th>\n",
       "      <td>-3.655222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terrible</th>\n",
       "      <td>-3.972675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointing</th>\n",
       "      <td>-4.151440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>-4.717565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>574 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   coef\n",
       "delicious      5.730341\n",
       "amazing        5.246080\n",
       "great          5.080927\n",
       "love           4.471123\n",
       "awesome        3.967118\n",
       "...                 ...\n",
       "mediocre      -3.638701\n",
       "rude          -3.655222\n",
       "terrible      -3.972675\n",
       "disappointing -4.151440\n",
       "bad           -4.717565\n",
       "\n",
       "[574 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the most informative words\n",
    "log_odds = logreg.coef_[0]\n",
    "coeff = pd.DataFrame(log_odds, X_train1_df.columns, columns=['coef']).sort_values(by='coef', ascending=False)\n",
    "coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd1db2e-3594-41da-88ce-d5220ed4b8d6",
   "metadata": {},
   "source": [
    "### Naive Bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1133f2f0-bbdb-4790-b44c-ccf09342356f",
   "metadata": {},
   "source": [
    "# Testing Sentiment Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ab8ab5d-be2e-45bb-8ab7-6b4b2a573206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special(text):\n",
    "    # remove the URL\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # remove mentions\n",
    "    text = re.sub(\"@[^\\s]*\", \"\", text)\n",
    "    # remove hashtags\n",
    "    text = re.sub(\"#[^\\s]*\", \"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def spacy_process(text):\n",
    "    doc = nlp(text)\n",
    "    # Lemmatization with Spacy\n",
    "    lemma_list = []\n",
    "    for token in doc:\n",
    "        lemma_list.append(token.lemma_)\n",
    "            \n",
    "    #Filter the stopwords, remove non-letters and lower case \n",
    "    #filtered_sentence =[]\n",
    "    #for word in lemma_list:\n",
    "        #lexeme = nlp.vocab[word]\n",
    "        #if lexeme.is_stop == False:\n",
    "            #filtered_sentence.append(word)\n",
    "    lower_words = []\n",
    "    for word in lemma_list:\n",
    "        filtered_aplha_char = re.sub(\"[^\\w]\" , \" \" , word)\n",
    "        filtered_single_n = re.sub(\"n\\sn\", \" \" ,filtered_aplha_char)\n",
    "        text_letters_only = re.sub(\"[^a-zA-Z]\", \" \", filtered_single_n)\n",
    "        text_words_lower = text_letters_only.lower()\n",
    "        remove_single_char = re.sub(r'\\b\\w\\b', '', text_words_lower)\n",
    "        lower_words.append(remove_single_char)\n",
    "    text_final = \" \".join(lower_words)\n",
    "    return  \" \".join(text_final.split())\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Stopwords\n",
    "    my_stop_words = set(stopwords.words('english') + \n",
    "                        list(ENGLISH_STOP_WORDS) + \n",
    "                        ['super', 'duper', 'place'])\n",
    "    exclude_stopwords = ['no','none']\n",
    "    for word in exclude_stopwords:\n",
    "        my_stop_words.remove(word)\n",
    "    \n",
    "    word_tokens = word_tokenize(text)\n",
    "    tokens_list = list()\n",
    "    for word in word_tokens:\n",
    "        if word.isalpha() and word not in my_stop_words:\n",
    "            tokens_list.append(word)\n",
    "    return tokens_list\n",
    "\n",
    "def clean_data(text, needed_format):\n",
    "    text = remove_special(text)\n",
    "    words_sentence = spacy_process(text)\n",
    "    tokens_list = remove_stopwords(words_sentence)\n",
    "    if needed_format == 'list':\n",
    "        return tokens_list\n",
    "    elif needed_format == 'string':\n",
    "        return words_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba72279c-8a19-40cc-8978-844638e2ea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_model(text):\n",
    "    print(\"Original review is:\\n\", text)\n",
    "    print(\"\\nCleaned review is:\\n\", clean_data(text, 'string'))\n",
    "    result = logreg.predict(vect_1.transform([clean_data(text,'string')]))\n",
    "    print(\"\\nLogistic Regression model: \", result)\n",
    "    if result == 0:\n",
    "        print(\"\\nThis review has negetive sentiment\\n\")\n",
    "    elif result == 1:\n",
    "        print(\"\\nThis review has positive sentiment\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46b1269e-42a1-4ac3-aa82-8ad88d437d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original review is:\n",
      " i like the pork burger, and the pizza is delicious, but the wait time is too long, I do not like burger, the place is bad\n",
      "\n",
      "Cleaned review is:\n",
      " like the pork burger and the pizza be delicious but the wait time be too long do not like burger the place be bad\n",
      "\n",
      "Logistic Regression model:  [0.]\n",
      "\n",
      "This review has negetive sentiment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"i like the pork burger, and the pizza is delicious, but the wait time is too long, I do not like burger, the place is bad\"\n",
    "testing_model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3cbafadc-3db7-4f1c-9a7a-a7a179116692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original review is:\n",
      "  people that works here for sure is friendly! :)I do love that big menu book and seems like there are a lot of items to choose from. This is always nice as Vietnamese food is definitely more than pho and more spring rolls.\n",
      "\n",
      "Cleaned review is:\n",
      " people that work here for sure be friendly do love that big menu book and seem like there be lot of item to choose from this be always nice as vietnamese food be definitely more than pho and more spring roll\n",
      "\n",
      "Logistic Regression model:  [1.]\n",
      "\n",
      "This review has positive sentiment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \" people that works here for sure is friendly! :)I do love that big menu book and seems like there are a lot of items to choose from. This is always nice as Vietnamese food is definitely more than pho and more spring rolls.\"\n",
    "testing_model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3ac829-f59c-4988-9cf7-6720c80ac5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
