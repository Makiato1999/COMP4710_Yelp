{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11a9669e-c92d-45ae-81b7-b59e2e8523af",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The previous stage includes reviews text preprocessing, which is in Process_Reviews.ipynb<br>\n",
    "The following stage is aim to analysis reviews sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9dbb51f-8804-4366-87b7-f9a3bc567865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\AA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\AA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\AA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
    "import seaborn as sns\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from gensim.models import Word2Vec\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import string\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2882f14-c15d-49e7-844b-49aa4a14dab9",
   "metadata": {},
   "source": [
    "# Load reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "788b8514-5d9e-442a-b96c-11e5d8bf35ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fxWnU4OqONBNoQhEcyazSg</td>\n",
       "      <td>krTHKI0YOpASr4gz2CVWFw</td>\n",
       "      <td>This location used to be good, several years a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>this location use to be good several year ago ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FhtER9SGsEYkEhRcs09rsQ</td>\n",
       "      <td>krTHKI0YOpASr4gz2CVWFw</td>\n",
       "      <td>I love Cosi but this Cosi is going down hill f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>love cosi but this cosi be go down hill fast a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0KlwfaHZyvao41_3S47dyg</td>\n",
       "      <td>w9hS5x1F52Id-G1KTrAOZg</td>\n",
       "      <td>Was not a fan of their cheesesteak. Their wiz ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>be not fan of their cheesesteak their wiz sauc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2qeje7dttkvREbccHev6Pg</td>\n",
       "      <td>7lwe7n-Yc-V9E_HfLAeylg</td>\n",
       "      <td>It pains me to write this, but I fear I must.....</td>\n",
       "      <td>0.0</td>\n",
       "      <td>it pain to write this but fear must use to rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1OR23O0giNcxNbFAi4jgcg</td>\n",
       "      <td>DsKzHnkLKnxZTVsFpts4oA</td>\n",
       "      <td>Cocktails were nice however the bartender Paul...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cocktail be nice however the bartender paul be...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id             business_id  \\\n",
       "0  fxWnU4OqONBNoQhEcyazSg  krTHKI0YOpASr4gz2CVWFw   \n",
       "1  FhtER9SGsEYkEhRcs09rsQ  krTHKI0YOpASr4gz2CVWFw   \n",
       "2  0KlwfaHZyvao41_3S47dyg  w9hS5x1F52Id-G1KTrAOZg   \n",
       "3  2qeje7dttkvREbccHev6Pg  7lwe7n-Yc-V9E_HfLAeylg   \n",
       "4  1OR23O0giNcxNbFAi4jgcg  DsKzHnkLKnxZTVsFpts4oA   \n",
       "\n",
       "                                                text  target  \\\n",
       "0  This location used to be good, several years a...     0.0   \n",
       "1  I love Cosi but this Cosi is going down hill f...     0.0   \n",
       "2  Was not a fan of their cheesesteak. Their wiz ...     0.0   \n",
       "3  It pains me to write this, but I fear I must.....     0.0   \n",
       "4  Cocktails were nice however the bartender Paul...     0.0   \n",
       "\n",
       "                                               words  \n",
       "0  this location use to be good several year ago ...  \n",
       "1  love cosi but this cosi be go down hill fast a...  \n",
       "2  be not fan of their cheesesteak their wiz sauc...  \n",
       "3  it pain to write this but fear must use to rea...  \n",
       "4  cocktail be nice however the bartender paul be...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"../Datasets/Cleaned_Text_Dataset.csv\"\n",
    "df = pd.read_csv(file)\n",
    "del df[\"Unnamed: 0\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078c8fdd-8a0d-4442-a015-8af277852562",
   "metadata": {},
   "source": [
    "# Sentiment polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2cd247-9cb6-46e4-b248-173d6783995a",
   "metadata": {},
   "source": [
    "我们将情绪分为四个类别，negative、neutral、positive and compound。【摘抄】The first three are easy to understand and for the compound score, it is a combination of positive and negative scores and ranges from -1 to 1: below 0 is negative and above 0 is positive. I am going to use the compound score to measure the sentiment.前三个很容易理解，对于复合分数，它是正分数和负分数的组合，范围从 -1 到 1：低于 0 为负，高于 0 为正。我将使用复合分数来衡量情绪。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34ee8ad8-2b95-4007-88e3-16e46451104c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this location use to be good several year ago about year ago it start to go downhill and now it be just terrible there be people work on saturday evening nearly all the table be full and there be people wait to order and to pay the sandwich that be make for be on hard bread burn and barely edible they need to either shape up or ship out\n",
      "0    {'neg': 0.08, 'neu': 0.881, 'pos': 0.039, 'com...\n",
      "1    {'neg': 0.253, 'neu': 0.702, 'pos': 0.046, 'co...\n",
      "2    {'neg': 0.11, 'neu': 0.813, 'pos': 0.077, 'com...\n",
      "3    {'neg': 0.22, 'neu': 0.62, 'pos': 0.16, 'compo...\n",
      "4    {'neg': 0.113, 'neu': 0.734, 'pos': 0.153, 'co...\n",
      "Name: words, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Instantiate new SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "# Generate sentiment scores\n",
    "sentiment_scores = df['words'].apply(sid.polarity_scores)\n",
    "sentiment = sentiment_scores.apply(lambda x: x['compound'])\n",
    "print(df['words'][0])\n",
    "print(sentiment_scores.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de3e6d1-391c-4aca-8288-f666f35d1170",
   "metadata": {},
   "source": [
    "# Split training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "787f4fcf-c4b2-4ca8-9038-f0e62d17d953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      " 1833    have read the review that this remind someone ...\n",
      "3046    sit by the huge window you have somewhat of ni...\n",
      "5958    tatiana the hostess sit we at wonderful table ...\n",
      "8688    really like this place it be one of the good p...\n",
      "2672    let set scene for you it be thursday and my bi...\n",
      "Name: words, dtype: object\n",
      "\n",
      "y_train:\n",
      " 1833    0.0\n",
      "3046    0.0\n",
      "5958    1.0\n",
      "8688    1.0\n",
      "2672    0.0\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X = df['words'] \n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify=y)\n",
    "print(\"X_train:\\n\", X_train.head())\n",
    "print(\"\\ny_train:\\n\", y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ff9c5-114e-4eaa-be38-904bbd8772b0",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fafc8d5-64e6-41dd-85e8-50609b72edbd",
   "metadata": {},
   "source": [
    "列举我们将要实验的n-gram，【摘抄】GridSearchCV是Sklearn model_selection包的一个模块，用于超参数调整。 给定一组不同的超参数，GridSearchCV 循环浏览所有可能的超参数值和组合，并在训练数据集上拟合模型。 在这个过程中，它能够确定产生最佳精度的超参数的最佳值和组合（从给定的参数集中）【摘抄】在机器学习模型中，需要人工选择的参数称为超参数。比如随机森林中决策树的个数，人工神经网络模型中隐藏层层数和每层的节点个数，正则项中常数大小等等，他们都需要事先指定。超参数选择不恰当，就会出现欠拟合或者过拟合的问题。而在选择超参数的时候，有两个途径，一个是凭经验微调，另一个就是选择不同大小的参数，带入模型中，挑选表现最好的参数。微调的一种方法是手工调制超参数，直到找到一个好的超参数组合，这么做的话会非常冗长，你也可能没有时间探索多种组合，所以可以使用Scikit-Learn的GridSearchCV来做这项搜索工作。<br>\n",
    "这里用的是后者<br>\n",
    "可以提一下交叉验证，cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fff587b-48fd-4467-9fc2-93b96f9b0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'c_vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e5488e-544e-4526-809d-8ff65a058dae",
   "metadata": {},
   "source": [
    "### Bag-of-words model(wordcounts) and Vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e4d16f-0d9c-49cf-bf4a-4797870f2ce2",
   "metadata": {},
   "source": [
    "### Logistic Regression model \n",
    "找出哪个n-gram在逻辑回归模型中表现更好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b330c3aa-8b88-42b0-96af-5944d3d4a76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal n-gram:  (1, 2)\n",
      "optimal parameter:  {'c_vectorizer__ngram_range': (1, 2)}\n",
      "optimal score:  0.9185685098212045\n",
      "classification report\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.942963</td>\n",
       "      <td>0.929197</td>\n",
       "      <td>0.936029</td>\n",
       "      <td>1370.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.930166</td>\n",
       "      <td>0.943755</td>\n",
       "      <td>0.936911</td>\n",
       "      <td>1369.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.936473</td>\n",
       "      <td>0.936473</td>\n",
       "      <td>0.936473</td>\n",
       "      <td>0.936473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.936564</td>\n",
       "      <td>0.936476</td>\n",
       "      <td>0.936470</td>\n",
       "      <td>2739.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.936567</td>\n",
       "      <td>0.936473</td>\n",
       "      <td>0.936470</td>\n",
       "      <td>2739.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0.0            0.942963  0.929197  0.936029  1370.000000\n",
       "1.0            0.930166  0.943755  0.936911  1369.000000\n",
       "accuracy       0.936473  0.936473  0.936473     0.936473\n",
       "macro avg      0.936564  0.936476  0.936470  2739.000000\n",
       "weighted avg   0.936567  0.936473  0.936470  2739.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_pipeline = Pipeline([\n",
    "    ('c_vectorizer', CountVectorizer()),\n",
    "    ('lr', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "gs_lr = GridSearchCV(lr_pipeline, refit=True, cv=2, param_grid=param_grid, scoring='f1', n_jobs=-1)\n",
    "gs_lr.fit(X_train, y_train)\n",
    "\n",
    "print('optimal n-gram: ', gs_lr.best_estimator_.get_params()['c_vectorizer__ngram_range'])\n",
    "print(\"optimal parameter: \", gs_lr.best_params_)\n",
    "print(\"optimal score: \", gs_lr.best_score_)\n",
    "\n",
    "print('classification report')\n",
    "predictions = gs_lr.predict(X_test)\n",
    "report = classification_report(y_test, predictions, digits=4, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51fba86-f294-415b-93fe-5569d2c80d6d",
   "metadata": {},
   "source": [
    "### Support Vector Machine model \n",
    "找出哪个n-gram在支持向量机SVM模型中表现更好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eb23476-d17a-4b5a-b2a2-9ff0dd7d53e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal n-gram:  (1, 2)\n",
      "optimal parameter:  {'c_vectorizer__ngram_range': (1, 2)}\n",
      "optimal score:  0.8775782788523114\n",
      "classification report\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.903860</td>\n",
       "      <td>0.905839</td>\n",
       "      <td>0.904849</td>\n",
       "      <td>1370.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.905564</td>\n",
       "      <td>0.903579</td>\n",
       "      <td>0.904570</td>\n",
       "      <td>1369.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.904710</td>\n",
       "      <td>0.904710</td>\n",
       "      <td>0.904710</td>\n",
       "      <td>0.90471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.904712</td>\n",
       "      <td>0.904709</td>\n",
       "      <td>0.904710</td>\n",
       "      <td>2739.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.904712</td>\n",
       "      <td>0.904710</td>\n",
       "      <td>0.904710</td>\n",
       "      <td>2739.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0.0            0.903860  0.905839  0.904849  1370.00000\n",
       "1.0            0.905564  0.903579  0.904570  1369.00000\n",
       "accuracy       0.904710  0.904710  0.904710     0.90471\n",
       "macro avg      0.904712  0.904709  0.904710  2739.00000\n",
       "weighted avg   0.904712  0.904710  0.904710  2739.00000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_pipe = Pipeline([\n",
    "    ('c_vectorizer', CountVectorizer()),\n",
    "    ('svm', svm.SVC(max_iter=-1, random_state=42))\n",
    "])\n",
    "\n",
    "gs_svm = GridSearchCV(svm_pipe, refit=True, cv=2, param_grid=param_grid, scoring='f1', n_jobs=-1)\n",
    "gs_svm.fit(X_train, y_train)\n",
    "\n",
    "print('optimal n-gram: ', gs_svm.best_estimator_.get_params()['c_vectorizer__ngram_range'])\n",
    "print(\"optimal parameter: \", gs_svm.best_params_)\n",
    "print(\"optimal score: \", gs_svm.best_score_)\n",
    "\n",
    "print('classification report')\n",
    "predictions = gs_svm.predict(X_test)\n",
    "report = classification_report(y_test, predictions, digits=4, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f321f6-6eab-4db1-9045-1fc6c8428a0e",
   "metadata": {},
   "source": [
    "【这里要改】对比上面逻辑回归和SVM交叉验证的结果，逻辑回归的最佳性能更好，因此我们选择最佳性能更好的逻辑回归结果，它的最优参数n-gram是（1，2）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0b050c-dae0-47fa-a08d-90370811bfc6",
   "metadata": {},
   "source": [
    "# Supervised Learning Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfbccbb3-2df8-423f-a724-773de613b8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      " 1833    have read the review that this remind someone ...\n",
      "3046    sit by the huge window you have somewhat of ni...\n",
      "5958    tatiana the hostess sit we at wonderful table ...\n",
      "8688    really like this place it be one of the good p...\n",
      "2672    let set scene for you it be thursday and my bi...\n",
      "Name: words, dtype: object\n",
      "\n",
      "y_train:\n",
      " 1833    0.0\n",
      "3046    0.0\n",
      "5958    1.0\n",
      "8688    1.0\n",
      "2672    0.0\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train:\\n\", X_train.head())\n",
    "print(\"\\ny_train:\\n\", y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e4b61-8673-4a42-89fe-da92301bbb77",
   "metadata": {},
   "source": [
    "### Bag-of-words model(TF-IDF) and Vectorisation\n",
    "【摘抄】we can use TF_IDF vectorizing to find the weighted words that occur more frequently in the document that leads to creation of the bag of words model我们可以使用 TF_IDF 向量化来找到文档中出现频率更高的加权词，从而创建词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eb00f8e-2d0c-4a68-956a-f9afb9df79a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of stop words with stopwords library \n",
    "# and adding extra stopwords that is not potentially useful \n",
    "my_stop_words = set(stopwords.words('english') + \n",
    "                    list(ENGLISH_STOP_WORDS) + \n",
    "                    ['super', 'duper', 've', 'like', 'got', \n",
    "                     'Cleveland', 'just', 'don', 'really', \n",
    "                     'said', 'told', 'ok','came', 'went', \n",
    "                     'did', 'didn', 'good'])\n",
    "#exclude_stopwords = ['no','none']\n",
    "#for word in exclude_stopwords:\n",
    "#    my_stop_words.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "605a213c-cbf0-4515-be64-8a1e96f26cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(sentence):\n",
    "    # to remove any space from beginning and the end of text\n",
    "    listofwords = sentence.strip().split()\n",
    "    listof_words = []    \n",
    "    for word in listofwords:\n",
    "        if not word in my_stop_words:\n",
    "            lemm_word = WordNetLemmatizer().lemmatize(word)\n",
    "            # remove the stop words\n",
    "            for punctuation_mark in string.punctuation:\n",
    "                word = word.replace(punctuation_mark, '').lower()\n",
    "            if len(word)>0:\n",
    "                listof_words.append(word)\n",
    "    return listof_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16111c-9121-4c52-81e4-f1d1cc699eae",
   "metadata": {},
   "source": [
    "从上面cross-validation得到最优的n-gram的结果(1,2)，在这里使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0158ec29-fce4-4c92-9c6f-f361ac00bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_1 = TfidfVectorizer(min_df=100,\n",
    "                         tokenizer=my_tokenizer,\n",
    "                         stop_words=list(my_stop_words), \n",
    "                         ngram_range=(1,2)).fit(X_train)\n",
    "X_train1 = vect_1.transform(X_train)\n",
    "X_test1 = vect_1.transform(X_test)\n",
    "# the below line for future coeff\n",
    "X_train1_df = pd.DataFrame(X_train1.toarray(), columns=vect_1.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ab430c2-155b-4a73-8a83-cbdd6d5b2c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>ago</th>\n",
       "      <th>amazing</th>\n",
       "      <th>ambiance</th>\n",
       "      <th>apologize</th>\n",
       "      <th>app</th>\n",
       "      <th>appetizer</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>worth</th>\n",
       "      <th>wow</th>\n",
       "      <th>wrap</th>\n",
       "      <th>write</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>yelp</th>\n",
       "      <th>yes</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.131165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114851</td>\n",
       "      <td>0.120613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.134618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6386</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.514717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6387</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184572</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6388</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6389</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.113224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.144276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.348614</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6390</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6391 rows × 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      able  absolutely  actually       add       ago  amazing  ambiance  \\\n",
       "0      0.0         0.0  0.131165  0.000000  0.000000      0.0       0.0   \n",
       "1      0.0         0.0  0.000000  0.000000  0.000000      0.0       0.0   \n",
       "2      0.0         0.0  0.000000  0.000000  0.000000      0.0       0.0   \n",
       "3      0.0         0.0  0.000000  0.000000  0.000000      0.0       0.0   \n",
       "4      0.0         0.0  0.000000  0.000000  0.000000      0.0       0.0   \n",
       "...    ...         ...       ...       ...       ...      ...       ...   \n",
       "6386   0.0         0.0  0.000000  0.000000  0.514717      0.0       0.0   \n",
       "6387   0.0         0.0  0.000000  0.184572  0.000000      0.0       0.0   \n",
       "6388   0.0         0.0  0.000000  0.000000  0.000000      0.0       0.0   \n",
       "6389   0.0         0.0  0.113224  0.000000  0.000000      0.0       0.0   \n",
       "6390   0.0         0.0  0.000000  0.000000  0.000000      0.0       0.0   \n",
       "\n",
       "      apologize  app  appetizer  ...      work     worth  wow  wrap  write  \\\n",
       "0      0.000000  0.0        0.0  ...  0.114851  0.120613  0.0   0.0    0.0   \n",
       "1      0.000000  0.0        0.0  ...  0.000000  0.000000  0.0   0.0    0.0   \n",
       "2      0.000000  0.0        0.0  ...  0.000000  0.000000  0.0   0.0    0.0   \n",
       "3      0.000000  0.0        0.0  ...  0.000000  0.000000  0.0   0.0    0.0   \n",
       "4      0.000000  0.0        0.0  ...  0.000000  0.000000  0.0   0.0    0.0   \n",
       "...         ...  ...        ...  ...       ...       ...  ...   ...    ...   \n",
       "6386   0.000000  0.0        0.0  ...  0.000000  0.000000  0.0   0.0    0.0   \n",
       "6387   0.000000  0.0        0.0  ...  0.154578  0.000000  0.0   0.0    0.0   \n",
       "6388   0.000000  0.0        0.0  ...  0.000000  0.000000  0.0   0.0    0.0   \n",
       "6389   0.144276  0.0        0.0  ...  0.000000  0.000000  0.0   0.0    0.0   \n",
       "6390   0.000000  0.0        0.0  ...  0.000000  0.000000  0.0   0.0    0.0   \n",
       "\n",
       "         wrong  year  yelp  yes  yummy  \n",
       "0     0.134618   0.0   0.0  0.0    0.0  \n",
       "1     0.000000   0.0   0.0  0.0    0.0  \n",
       "2     0.000000   0.0   0.0  0.0    0.0  \n",
       "3     0.000000   0.0   0.0  0.0    0.0  \n",
       "4     0.000000   0.0   0.0  0.0    0.0  \n",
       "...        ...   ...   ...  ...    ...  \n",
       "6386  0.000000   0.0   0.0  0.0    0.0  \n",
       "6387  0.000000   0.0   0.0  0.0    0.0  \n",
       "6388  0.000000   0.0   0.0  0.0    0.0  \n",
       "6389  0.348614   0.0   0.0  0.0    0.0  \n",
       "6390  0.000000   0.0   0.0  0.0    0.0  \n",
       "\n",
       "[6391 rows x 561 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_words = pd.DataFrame(columns=vect_1.get_feature_names_out(), data=X_train1.toarray())\n",
    "new_df_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d4fde2-3888-4fb1-b6f1-d82a850a18dd",
   "metadata": {},
   "source": [
    "【可以转csv，用作推荐】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89291af6-9728-42b5-9f2e-c0703b498b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>food</td>\n",
       "      <td>371.535738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>place</td>\n",
       "      <td>337.507768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>order</td>\n",
       "      <td>280.974762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>great</td>\n",
       "      <td>253.023882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>time</td>\n",
       "      <td>236.286287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>come</td>\n",
       "      <td>234.206358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>service</td>\n",
       "      <td>223.709631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>make</td>\n",
       "      <td>189.489262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>restaurant</td>\n",
       "      <td>175.841039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>try</td>\n",
       "      <td>169.702436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>eat</td>\n",
       "      <td>163.906174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>love</td>\n",
       "      <td>162.897940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>say</td>\n",
       "      <td>162.460409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>drink</td>\n",
       "      <td>158.718980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>chicken</td>\n",
       "      <td>145.782718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          words      counts\n",
       "172        food  371.535738\n",
       "357       place  337.507768\n",
       "332       order  280.974762\n",
       "194       great  253.023882\n",
       "498        time  236.286287\n",
       "83         come  234.206358\n",
       "430     service  223.709631\n",
       "284        make  189.489262\n",
       "397  restaurant  175.841039\n",
       "510         try  169.702436\n",
       "135         eat  163.906174\n",
       "276        love  162.897940\n",
       "419         say  162.460409\n",
       "130       drink  158.718980\n",
       "70      chicken  145.782718"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting most repetitive words \n",
    "word_counts = np.array(np.sum(X_train1, axis=0)).reshape((-1,))\n",
    "words = np.array(vect_1.get_feature_names_out())\n",
    "words_df = pd.DataFrame({\"words\":words, \"counts\":word_counts})\n",
    "words_df.sort_values(by=\"counts\",ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb8c694-f297-43f0-b72f-26a307e64c1a",
   "metadata": {},
   "source": [
    "### Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44233678-9df2-4853-a288-a6e8990945f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training set: 0.9145673603504929\n",
      "Score on test set: 0.8981380065717415\n"
     ]
    }
   ],
   "source": [
    "# Fitting Logistic regression to the training set\n",
    "logreg = LogisticRegression(solver='lbfgs',multi_class='auto',random_state=1)\n",
    "logreg.fit(X_train1, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred_logreg = logreg.predict(X_test1)\n",
    "\n",
    "# Training score\n",
    "print(f\"Score on training set: {logreg.score(X_train1,y_train)}\")\n",
    "print(f\"Score on test set: {logreg.score(X_test1,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af480924-8b22-4000-8b49-3e84d4d3ddf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True 0</th>\n",
       "      <td>1213</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 1</th>\n",
       "      <td>122</td>\n",
       "      <td>1247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Predicted 0  Predicted 1\n",
       "True 0         1213          157\n",
       "True 1          122         1247"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.908614</td>\n",
       "      <td>0.885401</td>\n",
       "      <td>0.896858</td>\n",
       "      <td>1370.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.888177</td>\n",
       "      <td>0.910884</td>\n",
       "      <td>0.899387</td>\n",
       "      <td>1369.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.898138</td>\n",
       "      <td>0.898138</td>\n",
       "      <td>0.898138</td>\n",
       "      <td>0.898138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.898395</td>\n",
       "      <td>0.898143</td>\n",
       "      <td>0.898122</td>\n",
       "      <td>2739.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.898399</td>\n",
       "      <td>0.898138</td>\n",
       "      <td>0.898122</td>\n",
       "      <td>2739.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0.0            0.908614  0.885401  0.896858  1370.000000\n",
       "1.0            0.888177  0.910884  0.899387  1369.000000\n",
       "accuracy       0.898138  0.898138  0.898138     0.898138\n",
       "macro avg      0.898395  0.898143  0.898122  2739.000000\n",
       "weighted avg   0.898399  0.898138  0.898122  2739.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('confusion matrix')\n",
    "con_mat_lr = confusion_matrix(y_test, y_pred_logreg)\n",
    "df_cm_lr = pd.DataFrame(con_mat_lr, columns = ['Predicted 0','Predicted 1'], index = ['True 0','True 1'])\n",
    "display(df_cm_lr)\n",
    "print('classification report')\n",
    "report = classification_report(y_test, y_pred_logreg, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04718d4f-375b-4943-b187-2478ede737ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>delicious</th>\n",
       "      <td>6.107103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>5.101902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>4.924239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>4.687776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>definitely</th>\n",
       "      <td>3.929150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horrible</th>\n",
       "      <td>-3.660362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bland</th>\n",
       "      <td>-3.683585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terrible</th>\n",
       "      <td>-3.789638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointing</th>\n",
       "      <td>-4.170694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>-4.274506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>561 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   coef\n",
       "delicious      6.107103\n",
       "amazing        5.101902\n",
       "love           4.924239\n",
       "great          4.687776\n",
       "definitely     3.929150\n",
       "...                 ...\n",
       "horrible      -3.660362\n",
       "bland         -3.683585\n",
       "terrible      -3.789638\n",
       "disappointing -4.170694\n",
       "bad           -4.274506\n",
       "\n",
       "[561 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the most informative words\n",
    "log_odds = logreg.coef_[0]\n",
    "coeff = pd.DataFrame(log_odds, X_train1_df.columns, columns=['coef']).sort_values(by='coef', ascending=False)\n",
    "coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd1db2e-3594-41da-88ce-d5220ed4b8d6",
   "metadata": {},
   "source": [
    "### Naive Bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1133f2f0-bbdb-4790-b44c-ccf09342356f",
   "metadata": {},
   "source": [
    "# Testing Sentiment Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ab8ab5d-be2e-45bb-8ab7-6b4b2a573206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special(text):\n",
    "    # remove the URL\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # remove mentions\n",
    "    text = re.sub(\"@[^\\s]*\", \"\", text)\n",
    "    # remove hashtags\n",
    "    text = re.sub(\"#[^\\s]*\", \"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def spacy_process(text):\n",
    "    doc = nlp(text)\n",
    "    # Lemmatization with Spacy\n",
    "    lemma_list = []\n",
    "    for token in doc:\n",
    "        lemma_list.append(token.lemma_)\n",
    "            \n",
    "    #Filter the stopwords, remove non-letters and lower case \n",
    "    #filtered_sentence =[]\n",
    "    #for word in lemma_list:\n",
    "        #lexeme = nlp.vocab[word]\n",
    "        #if lexeme.is_stop == False:\n",
    "            #filtered_sentence.append(word)\n",
    "    lower_words = []\n",
    "    for word in lemma_list:\n",
    "        filtered_aplha_char = re.sub(\"[^\\w]\" , \" \" , word)\n",
    "        filtered_single_n = re.sub(\"n\\sn\", \" \" ,filtered_aplha_char)\n",
    "        text_letters_only = re.sub(\"[^a-zA-Z]\", \" \", filtered_single_n)\n",
    "        text_words_lower = text_letters_only.lower()\n",
    "        remove_single_char = re.sub(r'\\b\\w\\b', '', text_words_lower)\n",
    "        lower_words.append(remove_single_char)\n",
    "    text_final = \" \".join(lower_words)\n",
    "    return  \" \".join(text_final.split())\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Stopwords\n",
    "    my_stop_words = set(stopwords.words('english') + \n",
    "                        list(ENGLISH_STOP_WORDS) + \n",
    "                        ['super', 'duper', 'place'])\n",
    "    exclude_stopwords = ['no','none']\n",
    "    for word in exclude_stopwords:\n",
    "        my_stop_words.remove(word)\n",
    "    \n",
    "    word_tokens = word_tokenize(text)\n",
    "    tokens_list = list()\n",
    "    for word in word_tokens:\n",
    "        if word.isalpha() and word not in my_stop_words:\n",
    "            tokens_list.append(word)\n",
    "    return tokens_list\n",
    "\n",
    "def clean_data(text, needed_format):\n",
    "    text = remove_special(text)\n",
    "    words_sentence = spacy_process(text)\n",
    "    tokens_list = remove_stopwords(words_sentence)\n",
    "    if needed_format == 'list':\n",
    "        return tokens_list\n",
    "    elif needed_format == 'string':\n",
    "        return words_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27e95af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_count</th>\n",
       "      <th>is_open</th>\n",
       "      <th>attributes</th>\n",
       "      <th>categories</th>\n",
       "      <th>hours</th>\n",
       "      <th>general_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hSbwd-VP4THYYvSKQQr6Ow</td>\n",
       "      <td>George's Famous Roast Pork and Beef</td>\n",
       "      <td>1007 S 9th St</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>PA</td>\n",
       "      <td>19147.0</td>\n",
       "      <td>39.937345</td>\n",
       "      <td>-75.158118</td>\n",
       "      <td>4.0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>{'RestaurantsReservations': 'False', 'Restaura...</td>\n",
       "      <td>Restaurants, Delis</td>\n",
       "      <td>{'Monday': '7:30-15:0', 'Tuesday': '7:30-15:0'...</td>\n",
       "      <td>Restaurants</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                                 name        address  \\\n",
       "0  hSbwd-VP4THYYvSKQQr6Ow  George's Famous Roast Pork and Beef  1007 S 9th St   \n",
       "\n",
       "           city state  postal_code   latitude  longitude  stars  review_count  \\\n",
       "0  Philadelphia    PA      19147.0  39.937345 -75.158118    4.0            27   \n",
       "\n",
       "   is_open                                         attributes  \\\n",
       "0        0  {'RestaurantsReservations': 'False', 'Restaura...   \n",
       "\n",
       "           categories                                              hours  \\\n",
       "0  Restaurants, Delis  {'Monday': '7:30-15:0', 'Tuesday': '7:30-15:0'...   \n",
       "\n",
       "  general_category  \n",
       "0      Restaurants  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the restaurant info file\n",
    "file = \"../Datasets/Filtered_Restaurant_Dataset.csv\"\n",
    "restaurant_df = pd.read_csv(file)\n",
    "restaurant_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec856594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_model(res_review):\n",
    "    review_bid = res_review['business_id']\n",
    "    review_rid = res_review['review_id']\n",
    "    review_res = restaurant_df[restaurant_df['business_id'] == review_bid]\n",
    "    review_res = review_res.reset_index(drop=True)\n",
    "    review = res_review['text']\n",
    "    review_cleaned = res_review['words']\n",
    "    review_res_name = review_res['name'][0]\n",
    "    print('Restaurant : ' + review_res_name)\n",
    "    print('-'*100)\n",
    "    print(\"Original review is:\\n\", review)\n",
    "    print(\"\\nCleaned review is:\\n\", review_cleaned)\n",
    "    result = logreg.predict(vect_1.transform([review_cleaned]))\n",
    "    print('-'*100)\n",
    "    print(\"\\nLogistic Regression model: \", result)\n",
    "    if result == 0:\n",
    "        print(\"\\nThis review has negetive sentiment\\n\")\n",
    "    elif result == 1:\n",
    "        print(\"\\nThis review has positive sentiment\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0eb2ace5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurant : Horizons\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Original review is:\n",
      " Ok. Now that was amazing. Cucumber avocado soup, vietnamese tempeh tacos, Pan seared tofu, soy cheesecake and wine from the southern hemisphere. Highly recommended\n",
      "\n",
      "Cleaned review is:\n",
      " ok now that be amazing cucumber avocado soup vietnamese tempeh tacos pan sear tofu soy cheesecake and wine from the southern hemisphere highly recommend\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Logistic Regression model:  [1.]\n",
      "\n",
      "This review has positive sentiment\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_res_review = df.loc[5111]\n",
    "testing_model(sample_res_review)\n",
    "sample_res_review['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c322d06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
